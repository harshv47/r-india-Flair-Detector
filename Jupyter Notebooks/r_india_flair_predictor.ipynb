{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "r/india flair predictor.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNNQCLzM+LBFcRZIEUt1QyJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshv47/r-india-Flair-Detector/blob/master/Jupyter%20Notebooks/r_india_flair_predictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rsx997lpu35",
        "colab_type": "text"
      },
      "source": [
        "## Importing Packages\n",
        "These are useful as later on, we will be calling functions from these libraries.\n",
        "\n",
        "Generally, I import the usual libraries first and then all model preproccesing specific libraries are imported in their respective cells as that makes it easier if I have to copy and test them elsewhere.\n",
        "\n",
        "The list of libraries that I import here is also relatively unchanged accross all projects that I do"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUv4VEPtlwOy",
        "colab_type": "code",
        "outputId": "481cb7af-7a22-4257-cfc4-d935e73977c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import datetime\n",
        "from scipy import stats\n",
        "\n",
        "nltk.download(\"stopwords\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYR6upmQqvHw",
        "colab_type": "text"
      },
      "source": [
        "## Praw and libs for downloading the dataset\n",
        "To download dataset from reddit, I am using praw. It is a wrapper for the reddit API. I had already made a project using praw so I recycled many parts of the code.\n",
        "\n",
        "Praw has built-in modules to access and download various parts of a subreddit, many of which are used here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "268bfdc6-adbd-43c7-a688-7098df18b7fd",
        "id": "_YlrgX-EkX_3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "!pip3 install praw\n",
        "import praw\n",
        "import os\n",
        "import datetime"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: praw in /usr/local/lib/python3.6/dist-packages (7.0.0)\n",
            "Requirement already satisfied: update-checker>=0.16 in /usr/local/lib/python3.6/dist-packages (from praw) (0.16)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.6/dist-packages (from praw) (0.57.0)\n",
            "Requirement already satisfied: prawcore<2.0,>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from praw) (1.3.0)\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from update-checker>=0.16->praw) (2.21.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from websocket-client>=0.54.0->praw) (1.12.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.16->praw) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.16->praw) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.16->praw) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.16->praw) (2.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEb62qxYsWKz",
        "colab_type": "text"
      },
      "source": [
        "## Set-up for Praw module\n",
        "Praw requires username and password of a reddit account as well as specific client ID, client secret and user_agent name; the later three have to be registered on reddit preferences under script tab.\n",
        "\n",
        "I re-used the credentials for my previous project([reddit_back](https://https://github.com/harshv47/Reddit-Background)) in this one. \n",
        "\n",
        "This fuction returns an object that can then be used to crawl reddit and is colloquially named reddit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXxnS-EUoTMi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def setUp(usrnm, passwd, cl_id, cl_sc):\n",
        "    print('Getting Reddit Data...')\n",
        "    reddit = praw.Reddit(client_id=cl_id, client_secret=cl_sc, user_agent='reddit_back', username=usrnm, password=passwd)\n",
        "\n",
        "    return reddit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4skYoUKt39h",
        "colab_type": "text"
      },
      "source": [
        "## Main Extract data function\n",
        "This is the function that crawl r/india and collects the dataset to be used.\n",
        "\n",
        "The name of all the flairs are saved in *flairs* list and the column names of our eventual dataset are stored in *topics*.\n",
        "Then, we navigate to r/india using the subreddit method.\n",
        "\n",
        "While collecting data, I first loop on *flairs* and then search & download data for the first 500 posts belonging to that flair. This was done to reduce bias in the dataset, so our dataset contains almost equal posts for each flair with the exception of few.\n",
        "\n",
        "For comments, I used only top level comments: these are the parent comments of each thread on reddit. After getting each comment I concatonate them in a single column, which would later be preproccesed.\n",
        "\n",
        "Finally this will be written as a csv file and saved so that it can be reused and one won't need to download it every time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-fJAS2QoUR_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extractData(reddit):\n",
        "    reddit.read_only = True\n",
        "    subreddit = reddit.subreddit('india')\n",
        "\n",
        "    flairs = [\"Politics\",\n",
        "            \"Non-Political\",\n",
        "            \"[R]eddiquette\",\n",
        "            \"AskIndia\",\n",
        "            \"Policy/Economy\",\n",
        "            \"Business/Finance\",\n",
        "            \"Science/Technology\",\n",
        "            \"Scheduled\",\n",
        "            \"Sports\",\n",
        "            \"Food\",\n",
        "            \"AMA\",\n",
        "            \"Photography\",\n",
        "            \"CAA-NRC-NPR\",\n",
        "            \"Coronavirus\"]\n",
        "\n",
        "    topics = { \"author\":[],\n",
        "                    \"body\":[],\n",
        "                    \"comments\":[],\n",
        "                    \"comms_num\":[],\n",
        "                    \"flair\": [],\n",
        "                    \"id\": [],\n",
        "                    \"score\":[],\n",
        "                    \"title\": [],\n",
        "                    \"url\": [],\n",
        "                    \"created\":[]}\n",
        "    print('Collecting Flair Data...')\n",
        "    flair_count = 1\n",
        "    for flair in flairs:\n",
        "        submissions = subreddit.search(flair, limit=500)\n",
        "        for submission in submissions:\n",
        "            \n",
        "            topics[\"flair\"].append(flair)\n",
        "            topics[\"title\"].append(submission.title)\n",
        "            topics[\"score\"].append(submission.score)\n",
        "            topics[\"id\"].append(submission.id)\n",
        "            topics[\"url\"].append(submission.url)\n",
        "            topics[\"comms_num\"].append(submission.num_comments)\n",
        "            topics[\"created\"].append(submission.created)\n",
        "            topics[\"body\"].append(submission.selftext)\n",
        "            topics[\"author\"].append(submission.author)\n",
        "            \n",
        "            #   Remove comments that are accessed by clicking on More Comments, limit = 0 implies no clicking\n",
        "            submission.comments.replace_more(limit=0)\n",
        "            comment = ''\n",
        "            #   Only using top level comments\n",
        "            for top_level_comment in submission.comments:\n",
        "                comment = comment + ' ' + top_level_comment.body\n",
        "            topics[\"comments\"].append(comment)\n",
        "        print('Collected flair: ',flair_count,' ',flair,' out of 12')\n",
        "        flair_count = flair_count + 1\n",
        "\n",
        "    topics_df = pd.DataFrame(topics)\n",
        "    #   The created time is in Unix Time, convert it to timestamp before proceding\n",
        "    print('Done Collecting Data, writing as csv')\n",
        "    topics_df.to_csv('dataset.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSXwA2HdxDXt",
        "colab_type": "text"
      },
      "source": [
        "## Calling all praw functions and credentials\n",
        "In this block all the credentials are stored in one place and then the functions that were defined above are called.\n",
        "\n",
        "It should be noted that as the running time of the *extractData* function is a bit high. I downloaded this using almost the same code albiet in a single python file stored in **/dataset/procure.py**. The file works a bit differently, it asking for the creds one time and stores it on the local host. That method is not displayed here because colab does not allow reading the file system and getting input inline in colab is a bit tacky."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sj9-PNJcofso",
        "colab_type": "code",
        "outputId": "d7651109-4587-4ada-8f6a-0716ed8a9ac7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "\n",
        "client_id = '#'\n",
        "client_secret = '#'\n",
        "username = '$'\n",
        "password = '$'\n",
        "\n",
        "#\tSetting up reddit, :\n",
        "reddit = setUp(username, password, client_id, client_secret)\n",
        "#   Main functon here:\n",
        "extractData(reddit)\n",
        "print('Done')"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Getting Reddit Data...\n",
            "Collecting Flair Data...\n",
            "Collected flair:  1   Politics  out of 12\n",
            "Collected flair:  2   Non-Political  out of 12\n",
            "Collected flair:  3   [R]eddiquette  out of 12\n",
            "Collected flair:  4   AskIndia  out of 12\n",
            "Collected flair:  5   Policy/Economy  out of 12\n",
            "Collected flair:  6   Business/Finance  out of 12\n",
            "Collected flair:  7   Science/Technology  out of 12\n",
            "Collected flair:  8   Scheduled  out of 12\n",
            "Collected flair:  9   Sports  out of 12\n",
            "Collected flair:  10   Food  out of 12\n",
            "Collected flair:  11   AMA  out of 12\n",
            "Collected flair:  12   Photography  out of 12\n",
            "Collected flair:  13   CAA-NRC-NPR  out of 12\n",
            "Collected flair:  14   Coronavirus  out of 12\n",
            "Done Collecting Data, writing as csv\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uG-dh_kPydmU",
        "colab_type": "text"
      },
      "source": [
        "## Importing saved dataset\n",
        "The dataset is importing as a DataFrame using pandas *read_csv* function.\n",
        "\n",
        "I also checked it once by using *head()* property of a DataFrame to see if the dataset looks correct."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDoxszeir5qo",
        "colab_type": "code",
        "outputId": "d03f26d4-d819-4c30-90fc-c2e509719e4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "dataset_df = pd.read_csv('dataset.csv')\n",
        "dataset_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>body</th>\n",
              "      <th>comments</th>\n",
              "      <th>comms_num</th>\n",
              "      <th>flair</th>\n",
              "      <th>id</th>\n",
              "      <th>score</th>\n",
              "      <th>title</th>\n",
              "      <th>url</th>\n",
              "      <th>created</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>aaluinsonaout</td>\n",
              "      <td>I don't know if it is the same situation in ot...</td>\n",
              "      <td>Our society thrives on abuse of power. We let...</td>\n",
              "      <td>82</td>\n",
              "      <td>Politics</td>\n",
              "      <td>g2ct57</td>\n",
              "      <td>405</td>\n",
              "      <td>A polite request to all Indians here</td>\n",
              "      <td>https://www.reddit.com/r/india/comments/g2ct57...</td>\n",
              "      <td>1.587063e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>HairLikeWinterFire</td>\n",
              "      <td>TLDR: My (unqualified) opinion is that dalit p...</td>\n",
              "      <td>I don't really get along with \"government upl...</td>\n",
              "      <td>20</td>\n",
              "      <td>Politics</td>\n",
              "      <td>g76o5f</td>\n",
              "      <td>30</td>\n",
              "      <td>The real loser in India's errupting Islamaphob...</td>\n",
              "      <td>https://www.reddit.com/r/india/comments/g76o5f...</td>\n",
              "      <td>1.587756e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>chillinvillain122</td>\n",
              "      <td>First of all let me start by saying it was stu...</td>\n",
              "      <td>Our country is just too far in at the moment ...</td>\n",
              "      <td>73</td>\n",
              "      <td>Politics</td>\n",
              "      <td>futac9</td>\n",
              "      <td>194</td>\n",
              "      <td>Pitting a community against a political party ...</td>\n",
              "      <td>https://www.reddit.com/r/india/comments/futac9...</td>\n",
              "      <td>1.586034e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>aaluinsonaout</td>\n",
              "      <td>NaN</td>\n",
              "      <td>This looks like an IIPM ad 1. Where did they ...</td>\n",
              "      <td>146</td>\n",
              "      <td>Politics</td>\n",
              "      <td>ff8sth</td>\n",
              "      <td>738</td>\n",
              "      <td>A new political party gave a full front page a...</td>\n",
              "      <td>https://i.redd.it/yjo9wpy38el41.jpg</td>\n",
              "      <td>1.583678e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>hipporama</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Well, Some people really deserve to die. ~~/s...</td>\n",
              "      <td>67</td>\n",
              "      <td>Politics</td>\n",
              "      <td>fpaj1w</td>\n",
              "      <td>407</td>\n",
              "      <td>Hit by backlash over posts on lack of medical ...</td>\n",
              "      <td>https://theprint.in/india/hit-by-backlash-over...</td>\n",
              "      <td>1.585254e+09</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               author  ...       created\n",
              "0       aaluinsonaout  ...  1.587063e+09\n",
              "1  HairLikeWinterFire  ...  1.587756e+09\n",
              "2   chillinvillain122  ...  1.586034e+09\n",
              "3       aaluinsonaout  ...  1.583678e+09\n",
              "4           hipporama  ...  1.585254e+09\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7UXHVX6zFuE",
        "colab_type": "text"
      },
      "source": [
        "## Additional Info #1\n",
        "I used the *shape* property of a DataFrame to see it's number of features and rows and also see the data types of each feature in the dataset by using *dtypes* property of a DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ookBEoMaUfsR",
        "colab_type": "code",
        "outputId": "06d2eb96-a433-4905-a0dc-10c5e4f99fd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "print(dataset_df.shape)\n",
        "print(dataset_df.dtypes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2883, 10)\n",
            "author        object\n",
            "body          object\n",
            "comments      object\n",
            "comms_num      int64\n",
            "flair         object\n",
            "id            object\n",
            "score          int64\n",
            "title         object\n",
            "url           object\n",
            "created      float64\n",
            "dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Rn6dOJo04V3",
        "colab_type": "text"
      },
      "source": [
        "## Additional Info #2\n",
        "Here we see that not all flairs have the same number of examples, with Coronavirus havig the highest number due to recent pandemic.\n",
        "\n",
        "I printed this because if we had gotten 500 examples for each flairs, we should have 7000 examples for all 14 flairs. But we see in the above cell that we only have 2883 examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SuAWf6Zq5P3",
        "colab_type": "code",
        "outputId": "596de4cb-9c57-499b-a596-17610541f1ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "dataset_df['flair'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Coronavirus           248\n",
              "Politics              247\n",
              "Food                  242\n",
              "Scheduled             234\n",
              "Business/Finance      233\n",
              "Sports                231\n",
              "AskIndia              231\n",
              "Photography           222\n",
              "Science/Technology    221\n",
              "Policy/Economy        220\n",
              "Non-Political         216\n",
              "AMA                   213\n",
              "CAA-NRC-NPR           107\n",
              "[R]eddiquette          18\n",
              "Name: flair, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4Gc39zyabdC",
        "colab_type": "text"
      },
      "source": [
        "#Preproccesing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUIbe75U14PN",
        "colab_type": "text"
      },
      "source": [
        "## pip Requirements\n",
        "I need to use *tldextract* and *catboost* but they do not come by default in colab, so I have to install them using pip."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2fMN25xzqQO",
        "colab_type": "code",
        "outputId": "5d2f81d9-4121-444a-b4cc-e0c3a9c1318c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        }
      },
      "source": [
        "!pip3 install tldextract\n",
        "!pip install catboost"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tldextract in /usr/local/lib/python3.6/dist-packages (2.2.2)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.6/dist-packages (from tldextract) (1.5.1)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from tldextract) (2.21.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.6/dist-packages (from tldextract) (2.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from tldextract) (46.1.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from requests-file>=1.4->tldextract) (1.12.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.1.0->tldextract) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.1.0->tldextract) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.1.0->tldextract) (3.0.4)\n",
            "Requirement already satisfied: catboost in /usr/local/lib/python3.6/dist-packages (0.23)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from catboost) (1.12.0)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.0.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from catboost) (4.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from catboost) (3.2.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.18.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from catboost) (1.4.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->catboost) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->catboost) (2.8.1)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly->catboost) (1.3.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OipfZ3cO2Ln6",
        "colab_type": "text"
      },
      "source": [
        "## Getting Domains of urls\n",
        "URL in itself has not meaning, but depending upon which domain it comes from could be valuable.\n",
        "\n",
        "This function extractes domain names from urls"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Se0vtiBUzxEe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tldextract\n",
        "def getDomain(url):\n",
        "  ext = tldextract.extract(url)\n",
        "  return ext.domain"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-lc4Pz-2v1O",
        "colab_type": "text"
      },
      "source": [
        "## Handling illegal words\n",
        "Body, comments and title may contain words and symbols which would not useful to us.\n",
        "\n",
        "SO, here I used regex and beautiful soup to replace these (*[/(){}\\[\\]\\|@,;]*) and remove these ([^0-9a-z #+_]) symbols.\n",
        "\n",
        "I also replace stopwords with space(' ')."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlZwbf5xwAr6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "def clean_text(text):\n",
        "    text = BeautifulSoup(text, \"lxml\").text\n",
        "    text = text.lower()\n",
        "    space_sub = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "    remove_bad_sym = re.compile('[^0-9a-z #+_]')\n",
        "    STOPWORDS = set(stopwords.words('english'))\n",
        "    text = space_sub.sub(' ', text)\n",
        "    text = remove_bad_sym.sub('', text)\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EcxjLe54Nqr",
        "colab_type": "text"
      },
      "source": [
        "## Convert to string\n",
        "Intially the body, comments and title are all objects, they will have to be converted to string for *clean_text()* function to run. Moreover later on they have to be regarded as strings for further preproccesing.\n",
        "\n",
        "This function converts them to string(str)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgTOinyNrAuz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_str(text):\n",
        "  return str(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01jwtGl25E49",
        "colab_type": "text"
      },
      "source": [
        "## Calling previous functions\n",
        "In this block the previous funcions are called on the features that have to be converted to string and cleaned.\n",
        "\n",
        "The url is also converted to domain names.\n",
        "\n",
        "This all is done using the *apply()* property of a DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MnrHUMutKEd",
        "colab_type": "code",
        "outputId": "a6e8d10c-0e4e-488b-f1c8-eb238c02bdd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "cols_to_string = ['body', 'comments', 'title']\n",
        "for col in cols_to_string:\n",
        "  dataset_df[col] = dataset_df[col].apply(to_str)\n",
        "  print(col,\"->\" ,isinstance(dataset_df[col][0], str))\n",
        "  dataset_df[col] = dataset_df[col].apply(clean_text)\n",
        "\n",
        "dataset_df['url'] = dataset_df['url'].apply(getDomain)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "body -> True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://youtu.be/kBvIqVr__C0\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "comments -> True\n",
            "title -> True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jufmb0Vr56EN",
        "colab_type": "text"
      },
      "source": [
        "## Breaking dataset_df into X_train and Y_train\n",
        "The dataset has to be eventually broken into X and Y. The Y part will contain the *flair* column as that is our target and for the X part we will drop the *flair*, *id* and *created*.\n",
        "\n",
        "*id* is dropped because it has not meaningful information, reddit calculates ids are just permalinks of the posts that can be used to uniquely identify the post sitewide.\n",
        "\n",
        "*created* is just the unix timestamp of the time of creation of the post. I first trained using it, but it had vinsignificant bearing of the final classification so I decided to remove it. It can be included."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sORrqxC1Kp7",
        "colab_type": "code",
        "outputId": "3313e341-ff9a-4d34-8f99-76858405d16d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "Y_train = dataset_df['flair']\n",
        "X_train = dataset_df.drop(['flair', 'id', 'created'], axis = 1)\n",
        "\n",
        "print(\"X_train\", X_train.shape)\n",
        "print(\"Y_train\", Y_train.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train (2883, 7)\n",
            "Y_train (2883,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKqfkrSz7Ips",
        "colab_type": "text"
      },
      "source": [
        "## Normalization for Number of Comments and score\n",
        "The *score* and *comms_num* contain numbers that can range from 0 to very large. As such we have to normalize it. \n",
        "\n",
        "I wrote the normalization function. That plus one is added to change the range of numbers from (-1, 1) to (0, 2) and after dividing them by 2, the effective range will become (0,1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mODcbjXQAo1w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalization for comms_num and score\n",
        "from sklearn import preprocessing\n",
        "\n",
        "cols_to_normalize = ['comms_num', 'score']\n",
        "for col in cols_to_normalize:\n",
        "  X_train[col] = ((X_train[col] - X_train[col].mean()) / (X_train[col].max() - X_train[col].min()) + 1)/2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R78Rt4Tb8E_-",
        "colab_type": "text"
      },
      "source": [
        "Checking X_train for the previous block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lzdq8dOQWngh",
        "colab_type": "code",
        "outputId": "76e69611-9a21-4a74-eca5-fb40aa16f2f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "X_train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>body</th>\n",
              "      <th>comments</th>\n",
              "      <th>comms_num</th>\n",
              "      <th>score</th>\n",
              "      <th>title</th>\n",
              "      <th>url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>aaluinsonaout</td>\n",
              "      <td>dont know situation countries india seen lot o...</td>\n",
              "      <td>society thrives abuse power let many idiots ab...</td>\n",
              "      <td>0.501207</td>\n",
              "      <td>0.507448</td>\n",
              "      <td>polite request indians</td>\n",
              "      <td>reddit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>HairLikeWinterFire</td>\n",
              "      <td>tldr unqualified opinion dalit political movem...</td>\n",
              "      <td>dont really get along government uplifting bac...</td>\n",
              "      <td>0.498336</td>\n",
              "      <td>0.496972</td>\n",
              "      <td>real loser indias errupting islamaphobia caste...</td>\n",
              "      <td>reddit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>chillinvillain122</td>\n",
              "      <td>first let start saying stupid whatever muslims...</td>\n",
              "      <td>country far moment theres turning back best ho...</td>\n",
              "      <td>0.500791</td>\n",
              "      <td>0.501554</td>\n",
              "      <td>pitting community political party fucking stupid</td>\n",
              "      <td>reddit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>aaluinsonaout</td>\n",
              "      <td>nan</td>\n",
              "      <td>looks like iipm ad 1 get funds full page ads 2...</td>\n",
              "      <td>0.504171</td>\n",
              "      <td>0.516752</td>\n",
              "      <td>new political party gave full front page ad po...</td>\n",
              "      <td>redd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>hipporama</td>\n",
              "      <td>nan</td>\n",
              "      <td>well people really deserve die country fucking...</td>\n",
              "      <td>0.500513</td>\n",
              "      <td>0.507504</td>\n",
              "      <td>hit backlash posts lack medical gear doctors g...</td>\n",
              "      <td>theprint</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               author  ...       url\n",
              "0       aaluinsonaout  ...    reddit\n",
              "1  HairLikeWinterFire  ...    reddit\n",
              "2   chillinvillain122  ...    reddit\n",
              "3       aaluinsonaout  ...      redd\n",
              "4           hipporama  ...  theprint\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZw5heXC8XCD",
        "colab_type": "text"
      },
      "source": [
        "## Preproccesing for *author* and *url*\n",
        "The *author* and *url* could be taken as categorical fields. \n",
        "\n",
        "Because there are many users on reddit which follow and post mainly political related matters. Especially r/india, where a majority of news mostly political and economical are shared. So, there are many users who consistently post posts belonging to a particular flair.\n",
        "\n",
        "*ur* is also a major distinction. Sites like livelaw or space.com will often belong to a *flair*. It is highly unlikely that space.com will have a Political news. However, this is not always the case, eg. theprint could have political, non-political, etc. types of *flair*. But, it is an important feature, nonetheless.\n",
        "\n",
        "Therefore, I used *get_dummies()* and delete it's initial column, I could have used *OneHotEncoder* for the same, it won't matter much. *get_dummies* creates aditional columns proportional to the number of classes, with a value of 0 or 1 if they belong to that class.\n",
        "\n",
        "Also, due to some an [issue](https://stackoverflow.com/questions/58639104/duplicate-columns-from-pandas-get-dummies) some columns in author are repeated. This can create problems later on. So, I removed duplicated columns.\n",
        "\n",
        "The *X_train.columns.duplicated()* returns True for any column that is repeated, since *.loc* only selectes columns with their index having a True value, we invert the bool value by using *~* and then select only those columns having True on their indices in the column DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EciZt2Bp3mj5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = pd.concat([X_train, pd.get_dummies(X_train['url'], prefix='url_')], axis=1)\n",
        "X_train = pd.concat([X_train, pd.get_dummies(X_train['author'], prefix='author_')], axis=1)\n",
        "\n",
        "X_train = X_train.drop(['url', 'author'], axis=1)\n",
        "X_train = X_train.loc[:,~X_train.columns.duplicated()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPJ1Xe7HGHGd",
        "colab_type": "text"
      },
      "source": [
        "## Checking for duplicates\n",
        "I check for any duplicates features. The *value_counts()* sorts by descending order by default and since the first one is 1. So, there are no duplicate columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoOqVmh73Nn2",
        "colab_type": "code",
        "outputId": "aea06e0d-32a6-4b30-fd23-ef5beb3cdc55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "X_train.columns.T.value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "author__akimera             1\n",
              "author__bhodrolok           1\n",
              "author__DisposableMAYBE     1\n",
              "author__kanchudeep          1\n",
              "author__teninchclitoris     1\n",
              "                           ..\n",
              "author__ExaltFibs24         1\n",
              "author__nou_kar             1\n",
              "author__aguyfrominternet    1\n",
              "url__weather                1\n",
              "author__xx_yariel_xx        1\n",
              "Length: 2031, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfnOf12IIDC8",
        "colab_type": "text"
      },
      "source": [
        "## Creating a Corpus\n",
        "The body, comments and title features are still just strings.\n",
        "\n",
        "No, classifier can take strings as input. It must be transformed it into a form where a machine (classifier) can read.\n",
        "\n",
        "A very good way of doing that is to apply TF-IDF. To do that we must first build a corpus. Our corpus will consist of all the words from body, comments and title so we create a new DataFrame by merging all three features.\n",
        "\n",
        "Also, since I would be adding TF-IDF features so these three featueres need to be removed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7M0BonMomaI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = X_train['body'] + X_train['comments'] + X_train['title']\n",
        "corpus.head()\n",
        "X_train = X_train.drop(['body', 'comments', 'title'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWdTHyZFJNhO",
        "colab_type": "text"
      },
      "source": [
        "## Applying TF-IDF\n",
        "So built a corpus on the train set and then used *fit_transform* to transform the three feature data.\n",
        "\n",
        "Then the resulting featues were added to *X_train* by concatenating them.\n",
        "\n",
        "At the end, I checked if it worked by checking their shapes and taking a peek on the *X_train* reveals that it did."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAf0L90puF5Z",
        "colab_type": "code",
        "outputId": "52097f40-f750-4147-dd63-5c041115f7bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "tfidf_vect = TfidfVectorizer(max_features=10000, stop_words='english', min_df=1, binary=0, use_idf=1, smooth_idf=0, sublinear_tf=1)\n",
        "\n",
        "tfidf_vect_vectors = tfidf_vect.fit_transform(corpus.values.astype('U'))\n",
        "\n",
        "col_names = ['tfidf_' + s for s in tfidf_vect.get_feature_names()]\n",
        "\n",
        "col_tfidf_df = pd.DataFrame(tfidf_vect_vectors.todense(), columns=col_names)\n",
        "\n",
        "\n",
        "X_train = pd.concat([X_train, col_tfidf_df], axis=1)\n",
        "\n",
        "print(X_train.shape, Y_train.shape)\n",
        "X_train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2883, 12028) (2883,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comms_num</th>\n",
              "      <th>score</th>\n",
              "      <th>url__164.100.47.4</th>\n",
              "      <th>url__500px</th>\n",
              "      <th>url__aljazeera</th>\n",
              "      <th>url__altnews</th>\n",
              "      <th>url__ampproject</th>\n",
              "      <th>url__aninews</th>\n",
              "      <th>url__article-14</th>\n",
              "      <th>url__asiasociety</th>\n",
              "      <th>url__baatbiharki</th>\n",
              "      <th>url__bangaloremirror</th>\n",
              "      <th>url__barandbench</th>\n",
              "      <th>url__bbc</th>\n",
              "      <th>url__behance</th>\n",
              "      <th>url__betootaadvocate</th>\n",
              "      <th>url__bloomberg</th>\n",
              "      <th>url__bloombergquint</th>\n",
              "      <th>url__bnewsindia</th>\n",
              "      <th>url__business-standard</th>\n",
              "      <th>url__businessinsider</th>\n",
              "      <th>url__businesstoday</th>\n",
              "      <th>url__businessweek</th>\n",
              "      <th>url__caravanmagazine</th>\n",
              "      <th>url__cbc</th>\n",
              "      <th>url__circleofcricket</th>\n",
              "      <th>url__cisce</th>\n",
              "      <th>url__cnbc</th>\n",
              "      <th>url__cnbctv18</th>\n",
              "      <th>url__cnn</th>\n",
              "      <th>url__convozine</th>\n",
              "      <th>url__cpim</th>\n",
              "      <th>url__crefacto</th>\n",
              "      <th>url__dailyo</th>\n",
              "      <th>url__deccanchronicle</th>\n",
              "      <th>url__deccanherald</th>\n",
              "      <th>url__delhishelterboard</th>\n",
              "      <th>url__dnaindia</th>\n",
              "      <th>url__dw</th>\n",
              "      <th>url__ecimpey</th>\n",
              "      <th>...</th>\n",
              "      <th>tfidf_yep</th>\n",
              "      <th>tfidf_yes</th>\n",
              "      <th>tfidf_yesterday</th>\n",
              "      <th>tfidf_yesterdays</th>\n",
              "      <th>tfidf_yield</th>\n",
              "      <th>tfidf_yields</th>\n",
              "      <th>tfidf_yo</th>\n",
              "      <th>tfidf_yoga</th>\n",
              "      <th>tfidf_yogendra</th>\n",
              "      <th>tfidf_yogi</th>\n",
              "      <th>tfidf_yojana</th>\n",
              "      <th>tfidf_york</th>\n",
              "      <th>tfidf_you1</th>\n",
              "      <th>tfidf_youd</th>\n",
              "      <th>tfidf_youll</th>\n",
              "      <th>tfidf_young</th>\n",
              "      <th>tfidf_younger</th>\n",
              "      <th>tfidf_youngsters</th>\n",
              "      <th>tfidf_youre</th>\n",
              "      <th>tfidf_yourstory</th>\n",
              "      <th>tfidf_yourstorycom</th>\n",
              "      <th>tfidf_youth</th>\n",
              "      <th>tfidf_youths</th>\n",
              "      <th>tfidf_youtube</th>\n",
              "      <th>tfidf_youve</th>\n",
              "      <th>tfidf_yr</th>\n",
              "      <th>tfidf_yrs</th>\n",
              "      <th>tfidf_yt</th>\n",
              "      <th>tfidf_yup</th>\n",
              "      <th>tfidf_zealand</th>\n",
              "      <th>tfidf_zee</th>\n",
              "      <th>tfidf_zero</th>\n",
              "      <th>tfidf_zerodha</th>\n",
              "      <th>tfidf_zerorating</th>\n",
              "      <th>tfidf_zindagi</th>\n",
              "      <th>tfidf_zomato</th>\n",
              "      <th>tfidf_zombies</th>\n",
              "      <th>tfidf_zone</th>\n",
              "      <th>tfidf_zones</th>\n",
              "      <th>tfidf_zyada</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.501207</td>\n",
              "      <td>0.507448</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.053323</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.020054</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.026012</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.498336</td>\n",
              "      <td>0.496972</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.026478</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.026948</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.500791</td>\n",
              "      <td>0.501554</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.077111</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.035774</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.043516</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.504171</td>\n",
              "      <td>0.516752</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.025904</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.034195</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.500513</td>\n",
              "      <td>0.507504</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  12028 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   comms_num     score  url__164.100.47.4  ...  tfidf_zone  tfidf_zones  tfidf_zyada\n",
              "0   0.501207  0.507448                  0  ...         0.0          0.0          0.0\n",
              "1   0.498336  0.496972                  0  ...         0.0          0.0          0.0\n",
              "2   0.500791  0.501554                  0  ...         0.0          0.0          0.0\n",
              "3   0.504171  0.516752                  0  ...         0.0          0.0          0.0\n",
              "4   0.500513  0.507504                  0  ...         0.0          0.0          0.0\n",
              "\n",
              "[5 rows x 12028 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZuZw8AEKOLm",
        "colab_type": "text"
      },
      "source": [
        "## Splitting dataset for training and cross-eval\n",
        "For cross-eval, the dataset must be split into two parts.\n",
        "I used 20% of the dataset for cross-eval\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooBRddE3wQd8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test_cv, Y_train, Y_test_cv = train_test_split(X_train, Y_train, test_size=0.2, random_state=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb2cIscJKtqr",
        "colab_type": "text"
      },
      "source": [
        "Value_counts for Cross-eval Y set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1nvNJUh5Cos",
        "colab_type": "code",
        "outputId": "6700f7db-e878-45af-987f-6b0390b97618",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "Y_test_cv.value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Politics              65\n",
              "Photography           53\n",
              "Science/Technology    50\n",
              "Business/Finance      48\n",
              "Sports                46\n",
              "Policy/Economy        46\n",
              "Food                  45\n",
              "Scheduled             44\n",
              "Coronavirus           41\n",
              "AMA                   40\n",
              "Non-Political         39\n",
              "AskIndia              36\n",
              "CAA-NRC-NPR           21\n",
              "[R]eddiquette          3\n",
              "Name: flair, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MbCeZRvK3W4",
        "colab_type": "text"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syiNlyJvLswF",
        "colab_type": "text"
      },
      "source": [
        "## Desicion Tree\n",
        "They are a tree based classification algorithm. They are simple but quite powerful on classification problems."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRvlNW6ALbka",
        "colab_type": "code",
        "outputId": "86f033ae-8331-4fbe-d854-e8b5276b1d0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dt = DecisionTreeClassifier(random_state=10)\n",
        "dt.fit(X_train, Y_train)\n",
        "dt.score(X_test_cv, Y_test_cv)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6603119584055459"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evhYjbCtK6sh",
        "colab_type": "text"
      },
      "source": [
        "## Random Forest\n",
        "I used Random Forest because as ensemble learning algorithms, they are ideally suited for this type of classification. Since these types of dataset overfit quite easily.\n",
        "\n",
        "Random Forest doesn't overfit and the testing performance of Random Forests does not decrease (due to overfitting) as the number of trees increases. \n",
        "\n",
        "That is why I get better score than Descision Trees."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Frta2PL73GgO",
        "colab_type": "code",
        "outputId": "a53514e1-330d-4eb6-f1dd-41d1d1ca0858",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier(random_state=10)\n",
        "rf.fit(X_train, Y_train)\n",
        "rf.score(X_test_cv, Y_test_cv)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7435008665511266"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9V2Bk_eMvaE",
        "colab_type": "text"
      },
      "source": [
        "## SGDClassifier\n",
        "This is Logisitic regression optimized by Stochastic gradient descent.\n",
        "\n",
        "As we can see, the accuracy is less than Random Forest so it is a little worse for this task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxtn170eF353",
        "colab_type": "code",
        "outputId": "83c4c53d-560c-456e-c178-93e634e5cdd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "sgd = SGDClassifier(penalty='l2', loss='log', random_state=10)\n",
        "sgd.fit(X_train, Y_train)\n",
        "sgd.score(X_test_cv, Y_test_cv)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6811091854419411"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OP4ZdiQTOxEF",
        "colab_type": "text"
      },
      "source": [
        "## Linear SVC\n",
        "I have also used linear svc.\n",
        "\n",
        "Generally svm and derivates perform quite good in text classification problems. However, we find that it's score it lower than that of LR. I surmised that it might have been caused due to the *X_Train* matrix being quite sparse due to *get_dummies* and *TF-IDF*. This is also evident by it's relatively longer training time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZrMvkHWBkUs",
        "colab_type": "code",
        "outputId": "98dc767a-3419-4fe9-b223-6f07ed45c69d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn import svm\n",
        "svm_linear = svm.SVC(kernel='linear',random_state = 10).fit(X_train, Y_train)\n",
        "svm_linear.score(X_test_cv, Y_test_cv)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6707105719237435"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pcj_ER0PwFA",
        "colab_type": "text"
      },
      "source": [
        "## Code for Ensemble\n",
        "The code is for taking the mean of the probabilities for each class summed across all the models and at the end mapping them to the appropriate prediction class.\n",
        "\n",
        "It can work with any classification algorithm that has *predict_proba* as their property. Notably, SGD with hinge loss, doesn't have this property. And, to enable this in svm (for Linear SVC) one has to add a parameter *Probability=True*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31DQ8Ou8_slP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ensemble_predictions(members,n_members, testX):\n",
        "  # make predictions\n",
        "  yhats = [model.predict_proba(testX) for model in members]\n",
        "  yhats = np.array(yhats)\n",
        "  \n",
        "  # sum across ensemble members\n",
        "  summed = np.mean(yhats, axis=0)\n",
        "  # argmax across classes\n",
        "  result = [members[0].classes_[np.argmax(row)] for row in summed]\n",
        "  \n",
        "  return result\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5G7Tn2CcQt_1",
        "colab_type": "text"
      },
      "source": [
        "## Ensemble\n",
        "The accuray of Ensemble is notably less than that of Random Forest which means that sgd moves the prediction away from the Random Forest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-7r9TuJG63x",
        "colab_type": "code",
        "outputId": "79c761ad-6fd4-44e2-aa11-9a2046ae340b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(Y_test_cv, ensemble_predictions([rf, sgd], 2, X_test_cv))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7261698440207972"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRhqhaOVRdmN",
        "colab_type": "text"
      },
      "source": [
        "## CatBoost\n",
        "Until now I have only used Bagging Ensemble algorithm, CatBoost is a boosting ensemble algorithm.\n",
        "\n",
        "It is very powerful and can intelligently find the best model for a given task as well as the best iteration.\n",
        "\n",
        "However, it has the downside of taking good amount of time to train and due to it being a boosting ensemble, it can overfit.\n",
        "\n",
        "It does provide the best cross-eval score, though it might have overfitted a bit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhH088XhuO_T",
        "colab_type": "code",
        "outputId": "f6872ba4-9c3b-4583-c3d4-e5ac2a8ef24d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from catboost import CatBoostClassifier, Pool, cv\n",
        "catb=CatBoostClassifier(\n",
        "    iterations = 200,\n",
        "    random_seed = 10\n",
        "    )\n",
        "catb.fit(X_train,Y_train,eval_set=(X_test_cv, Y_test_cv))\n",
        "catb.score(X_test_cv,Y_test_cv)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate set to 0.203045\n",
            "0:\tlearn: 2.2797853\ttest: 2.3260453\tbest: 2.3260453 (0)\ttotal: 5.42s\tremaining: 17m 58s\n",
            "1:\tlearn: 2.0845554\ttest: 2.1274863\tbest: 2.1274863 (1)\ttotal: 10.1s\tremaining: 16m 43s\n",
            "2:\tlearn: 1.9057763\ttest: 1.9407153\tbest: 1.9407153 (2)\ttotal: 14.8s\tremaining: 16m 10s\n",
            "3:\tlearn: 1.7448231\ttest: 1.8057989\tbest: 1.8057989 (3)\ttotal: 19.4s\tremaining: 15m 51s\n",
            "4:\tlearn: 1.6570032\ttest: 1.7052055\tbest: 1.7052055 (4)\ttotal: 24.2s\tremaining: 15m 44s\n",
            "5:\tlearn: 1.5751966\ttest: 1.6438718\tbest: 1.6438718 (5)\ttotal: 29.1s\tremaining: 15m 40s\n",
            "6:\tlearn: 1.4991610\ttest: 1.5761534\tbest: 1.5761534 (6)\ttotal: 33.9s\tremaining: 15m 35s\n",
            "7:\tlearn: 1.4340519\ttest: 1.5200192\tbest: 1.5200192 (7)\ttotal: 38.9s\tremaining: 15m 32s\n",
            "8:\tlearn: 1.3995713\ttest: 1.4829034\tbest: 1.4829034 (8)\ttotal: 43.7s\tremaining: 15m 26s\n",
            "9:\tlearn: 1.3515166\ttest: 1.4338222\tbest: 1.4338222 (9)\ttotal: 48.5s\tremaining: 15m 21s\n",
            "10:\tlearn: 1.3199014\ttest: 1.3979953\tbest: 1.3979953 (10)\ttotal: 53.5s\tremaining: 15m 18s\n",
            "11:\tlearn: 1.2902766\ttest: 1.3706055\tbest: 1.3706055 (11)\ttotal: 58.4s\tremaining: 15m 14s\n",
            "12:\tlearn: 1.2591968\ttest: 1.3378479\tbest: 1.3378479 (12)\ttotal: 1m 3s\tremaining: 15m 11s\n",
            "13:\tlearn: 1.2188384\ttest: 1.3026704\tbest: 1.3026704 (13)\ttotal: 1m 8s\tremaining: 15m 7s\n",
            "14:\tlearn: 1.1905664\ttest: 1.2705081\tbest: 1.2705081 (14)\ttotal: 1m 13s\tremaining: 15m 3s\n",
            "15:\tlearn: 1.1718339\ttest: 1.2536237\tbest: 1.2536237 (15)\ttotal: 1m 18s\tremaining: 14m 59s\n",
            "16:\tlearn: 1.1517023\ttest: 1.2406783\tbest: 1.2406783 (16)\ttotal: 1m 23s\tremaining: 14m 55s\n",
            "17:\tlearn: 1.1319060\ttest: 1.2222277\tbest: 1.2222277 (17)\ttotal: 1m 28s\tremaining: 14m 50s\n",
            "18:\tlearn: 1.1118908\ttest: 1.2079897\tbest: 1.2079897 (18)\ttotal: 1m 33s\tremaining: 14m 46s\n",
            "19:\tlearn: 1.0970268\ttest: 1.1962161\tbest: 1.1962161 (19)\ttotal: 1m 37s\tremaining: 14m 41s\n",
            "20:\tlearn: 1.0831319\ttest: 1.1822720\tbest: 1.1822720 (20)\ttotal: 1m 42s\tremaining: 14m 36s\n",
            "21:\tlearn: 1.0697388\ttest: 1.1702510\tbest: 1.1702510 (21)\ttotal: 1m 47s\tremaining: 14m 31s\n",
            "22:\tlearn: 1.0574949\ttest: 1.1584353\tbest: 1.1584353 (22)\ttotal: 1m 52s\tremaining: 14m 26s\n",
            "23:\tlearn: 1.0440004\ttest: 1.1474399\tbest: 1.1474399 (23)\ttotal: 1m 57s\tremaining: 14m 20s\n",
            "24:\tlearn: 1.0348448\ttest: 1.1386541\tbest: 1.1386541 (24)\ttotal: 2m 2s\tremaining: 14m 15s\n",
            "25:\tlearn: 1.0256936\ttest: 1.1298849\tbest: 1.1298849 (25)\ttotal: 2m 6s\tremaining: 14m 9s\n",
            "26:\tlearn: 1.0136378\ttest: 1.1220643\tbest: 1.1220643 (26)\ttotal: 2m 11s\tremaining: 14m 4s\n",
            "27:\tlearn: 1.0038277\ttest: 1.1174997\tbest: 1.1174997 (27)\ttotal: 2m 16s\tremaining: 13m 59s\n",
            "28:\tlearn: 0.9910438\ttest: 1.1061379\tbest: 1.1061379 (28)\ttotal: 2m 21s\tremaining: 13m 54s\n",
            "29:\tlearn: 0.9771970\ttest: 1.0969099\tbest: 1.0969099 (29)\ttotal: 2m 26s\tremaining: 13m 48s\n",
            "30:\tlearn: 0.9681574\ttest: 1.0883416\tbest: 1.0883416 (30)\ttotal: 2m 30s\tremaining: 13m 43s\n",
            "31:\tlearn: 0.9594767\ttest: 1.0829480\tbest: 1.0829480 (31)\ttotal: 2m 35s\tremaining: 13m 37s\n",
            "32:\tlearn: 0.9489185\ttest: 1.0751732\tbest: 1.0751732 (32)\ttotal: 2m 40s\tremaining: 13m 32s\n",
            "33:\tlearn: 0.9315957\ttest: 1.0615694\tbest: 1.0615694 (33)\ttotal: 2m 45s\tremaining: 13m 27s\n",
            "34:\tlearn: 0.9204114\ttest: 1.0535964\tbest: 1.0535964 (34)\ttotal: 2m 50s\tremaining: 13m 21s\n",
            "35:\tlearn: 0.9062195\ttest: 1.0411507\tbest: 1.0411507 (35)\ttotal: 2m 54s\tremaining: 13m 17s\n",
            "36:\tlearn: 0.8919875\ttest: 1.0302038\tbest: 1.0302038 (36)\ttotal: 2m 59s\tremaining: 13m 11s\n",
            "37:\tlearn: 0.8833103\ttest: 1.0216871\tbest: 1.0216871 (37)\ttotal: 3m 4s\tremaining: 13m 6s\n",
            "38:\tlearn: 0.8764141\ttest: 1.0159926\tbest: 1.0159926 (38)\ttotal: 3m 9s\tremaining: 13m 1s\n",
            "39:\tlearn: 0.8665394\ttest: 1.0088017\tbest: 1.0088017 (39)\ttotal: 3m 14s\tremaining: 12m 56s\n",
            "40:\tlearn: 0.8498379\ttest: 0.9938497\tbest: 0.9938497 (40)\ttotal: 3m 18s\tremaining: 12m 51s\n",
            "41:\tlearn: 0.8438705\ttest: 0.9924310\tbest: 0.9924310 (41)\ttotal: 3m 23s\tremaining: 12m 46s\n",
            "42:\tlearn: 0.8284991\ttest: 0.9772136\tbest: 0.9772136 (42)\ttotal: 3m 28s\tremaining: 12m 41s\n",
            "43:\tlearn: 0.8188938\ttest: 0.9711815\tbest: 0.9711815 (43)\ttotal: 3m 33s\tremaining: 12m 36s\n",
            "44:\tlearn: 0.8095358\ttest: 0.9609253\tbest: 0.9609253 (44)\ttotal: 3m 38s\tremaining: 12m 31s\n",
            "45:\tlearn: 0.8063336\ttest: 0.9579722\tbest: 0.9579722 (45)\ttotal: 3m 43s\tremaining: 12m 26s\n",
            "46:\tlearn: 0.8044239\ttest: 0.9569487\tbest: 0.9569487 (46)\ttotal: 3m 47s\tremaining: 12m 22s\n",
            "47:\tlearn: 0.7955933\ttest: 0.9516864\tbest: 0.9516864 (47)\ttotal: 3m 52s\tremaining: 12m 17s\n",
            "48:\tlearn: 0.7863968\ttest: 0.9430992\tbest: 0.9430992 (48)\ttotal: 3m 57s\tremaining: 12m 12s\n",
            "49:\tlearn: 0.7853572\ttest: 0.9432540\tbest: 0.9430992 (48)\ttotal: 4m 2s\tremaining: 12m 7s\n",
            "50:\tlearn: 0.7841422\ttest: 0.9437432\tbest: 0.9430992 (48)\ttotal: 4m 7s\tremaining: 12m 3s\n",
            "51:\tlearn: 0.7763810\ttest: 0.9379011\tbest: 0.9379011 (51)\ttotal: 4m 12s\tremaining: 11m 58s\n",
            "52:\tlearn: 0.7645647\ttest: 0.9307305\tbest: 0.9307305 (52)\ttotal: 4m 17s\tremaining: 11m 53s\n",
            "53:\tlearn: 0.7574066\ttest: 0.9251975\tbest: 0.9251975 (53)\ttotal: 4m 22s\tremaining: 11m 48s\n",
            "54:\tlearn: 0.7542231\ttest: 0.9238199\tbest: 0.9238199 (54)\ttotal: 4m 27s\tremaining: 11m 44s\n",
            "55:\tlearn: 0.7474081\ttest: 0.9180381\tbest: 0.9180381 (55)\ttotal: 4m 31s\tremaining: 11m 39s\n",
            "56:\tlearn: 0.7391722\ttest: 0.9142833\tbest: 0.9142833 (56)\ttotal: 4m 36s\tremaining: 11m 34s\n",
            "57:\tlearn: 0.7289457\ttest: 0.9070413\tbest: 0.9070413 (57)\ttotal: 4m 41s\tremaining: 11m 29s\n",
            "58:\tlearn: 0.7274213\ttest: 0.9062269\tbest: 0.9062269 (58)\ttotal: 4m 46s\tremaining: 11m 24s\n",
            "59:\tlearn: 0.7236586\ttest: 0.9013937\tbest: 0.9013937 (59)\ttotal: 4m 51s\tremaining: 11m 19s\n",
            "60:\tlearn: 0.7185338\ttest: 0.8980415\tbest: 0.8980415 (60)\ttotal: 4m 56s\tremaining: 11m 14s\n",
            "61:\tlearn: 0.7172087\ttest: 0.8973777\tbest: 0.8973777 (61)\ttotal: 5m 1s\tremaining: 11m 10s\n",
            "62:\tlearn: 0.7149027\ttest: 0.8959134\tbest: 0.8959134 (62)\ttotal: 5m 5s\tremaining: 11m 5s\n",
            "63:\tlearn: 0.7117966\ttest: 0.8939500\tbest: 0.8939500 (63)\ttotal: 5m 10s\tremaining: 11m\n",
            "64:\tlearn: 0.7066925\ttest: 0.8910479\tbest: 0.8910479 (64)\ttotal: 5m 15s\tremaining: 10m 55s\n",
            "65:\tlearn: 0.7050030\ttest: 0.8912333\tbest: 0.8910479 (64)\ttotal: 5m 20s\tremaining: 10m 50s\n",
            "66:\tlearn: 0.7014016\ttest: 0.8875346\tbest: 0.8875346 (66)\ttotal: 5m 25s\tremaining: 10m 46s\n",
            "67:\tlearn: 0.6990408\ttest: 0.8863778\tbest: 0.8863778 (67)\ttotal: 5m 30s\tremaining: 10m 41s\n",
            "68:\tlearn: 0.6968901\ttest: 0.8839205\tbest: 0.8839205 (68)\ttotal: 5m 35s\tremaining: 10m 36s\n",
            "69:\tlearn: 0.6920425\ttest: 0.8808170\tbest: 0.8808170 (69)\ttotal: 5m 40s\tremaining: 10m 31s\n",
            "70:\tlearn: 0.6913662\ttest: 0.8807866\tbest: 0.8807866 (70)\ttotal: 5m 45s\tremaining: 10m 27s\n",
            "71:\tlearn: 0.6884141\ttest: 0.8769222\tbest: 0.8769222 (71)\ttotal: 5m 50s\tremaining: 10m 22s\n",
            "72:\tlearn: 0.6861049\ttest: 0.8758087\tbest: 0.8758087 (72)\ttotal: 5m 55s\tremaining: 10m 17s\n",
            "73:\tlearn: 0.6854662\ttest: 0.8759479\tbest: 0.8758087 (72)\ttotal: 5m 59s\tremaining: 10m 12s\n",
            "74:\tlearn: 0.6843293\ttest: 0.8747024\tbest: 0.8747024 (74)\ttotal: 6m 4s\tremaining: 10m 8s\n",
            "75:\tlearn: 0.6805981\ttest: 0.8718619\tbest: 0.8718619 (75)\ttotal: 6m 9s\tremaining: 10m 3s\n",
            "76:\tlearn: 0.6796630\ttest: 0.8711885\tbest: 0.8711885 (76)\ttotal: 6m 14s\tremaining: 9m 58s\n",
            "77:\tlearn: 0.6791777\ttest: 0.8713517\tbest: 0.8711885 (76)\ttotal: 6m 19s\tremaining: 9m 53s\n",
            "78:\tlearn: 0.6755089\ttest: 0.8700697\tbest: 0.8700697 (78)\ttotal: 6m 24s\tremaining: 9m 48s\n",
            "79:\tlearn: 0.6730832\ttest: 0.8697408\tbest: 0.8697408 (79)\ttotal: 6m 29s\tremaining: 9m 43s\n",
            "80:\tlearn: 0.6704588\ttest: 0.8689730\tbest: 0.8689730 (80)\ttotal: 6m 34s\tremaining: 9m 39s\n",
            "81:\tlearn: 0.6696522\ttest: 0.8678242\tbest: 0.8678242 (81)\ttotal: 6m 39s\tremaining: 9m 34s\n",
            "82:\tlearn: 0.6673718\ttest: 0.8676005\tbest: 0.8676005 (82)\ttotal: 6m 43s\tremaining: 9m 29s\n",
            "83:\tlearn: 0.6638006\ttest: 0.8648665\tbest: 0.8648665 (83)\ttotal: 6m 48s\tremaining: 9m 24s\n",
            "84:\tlearn: 0.6632011\ttest: 0.8645366\tbest: 0.8645366 (84)\ttotal: 6m 53s\tremaining: 9m 19s\n",
            "85:\tlearn: 0.6617264\ttest: 0.8637509\tbest: 0.8637509 (85)\ttotal: 6m 58s\tremaining: 9m 14s\n",
            "86:\tlearn: 0.6609407\ttest: 0.8634543\tbest: 0.8634543 (86)\ttotal: 7m 4s\tremaining: 9m 11s\n",
            "87:\tlearn: 0.6574341\ttest: 0.8625088\tbest: 0.8625088 (87)\ttotal: 7m 11s\tremaining: 9m 9s\n",
            "88:\tlearn: 0.6547856\ttest: 0.8621405\tbest: 0.8621405 (88)\ttotal: 7m 16s\tremaining: 9m 4s\n",
            "89:\tlearn: 0.6535492\ttest: 0.8617426\tbest: 0.8617426 (89)\ttotal: 7m 21s\tremaining: 8m 59s\n",
            "90:\tlearn: 0.6492915\ttest: 0.8591727\tbest: 0.8591727 (90)\ttotal: 7m 26s\tremaining: 8m 54s\n",
            "91:\tlearn: 0.6480284\ttest: 0.8591292\tbest: 0.8591292 (91)\ttotal: 7m 30s\tremaining: 8m 49s\n",
            "92:\tlearn: 0.6448798\ttest: 0.8560759\tbest: 0.8560759 (92)\ttotal: 7m 35s\tremaining: 8m 44s\n",
            "93:\tlearn: 0.6425412\ttest: 0.8538498\tbest: 0.8538498 (93)\ttotal: 7m 40s\tremaining: 8m 39s\n",
            "94:\tlearn: 0.6419525\ttest: 0.8538050\tbest: 0.8538050 (94)\ttotal: 7m 45s\tremaining: 8m 34s\n",
            "95:\tlearn: 0.6407791\ttest: 0.8528195\tbest: 0.8528195 (95)\ttotal: 7m 50s\tremaining: 8m 29s\n",
            "96:\tlearn: 0.6371617\ttest: 0.8520812\tbest: 0.8520812 (96)\ttotal: 7m 55s\tremaining: 8m 24s\n",
            "97:\tlearn: 0.6346179\ttest: 0.8512780\tbest: 0.8512780 (97)\ttotal: 8m\tremaining: 8m 19s\n",
            "98:\tlearn: 0.6340206\ttest: 0.8512814\tbest: 0.8512780 (97)\ttotal: 8m 5s\tremaining: 8m 15s\n",
            "99:\tlearn: 0.6333173\ttest: 0.8512609\tbest: 0.8512609 (99)\ttotal: 8m 10s\tremaining: 8m 10s\n",
            "100:\tlearn: 0.6309749\ttest: 0.8516909\tbest: 0.8512609 (99)\ttotal: 8m 15s\tremaining: 8m 5s\n",
            "101:\tlearn: 0.6302991\ttest: 0.8514021\tbest: 0.8512609 (99)\ttotal: 8m 20s\tremaining: 8m\n",
            "102:\tlearn: 0.6295184\ttest: 0.8511806\tbest: 0.8511806 (102)\ttotal: 8m 25s\tremaining: 7m 55s\n",
            "103:\tlearn: 0.6291057\ttest: 0.8513290\tbest: 0.8511806 (102)\ttotal: 8m 30s\tremaining: 7m 50s\n",
            "104:\tlearn: 0.6282439\ttest: 0.8510952\tbest: 0.8510952 (104)\ttotal: 8m 34s\tremaining: 7m 45s\n",
            "105:\tlearn: 0.6272852\ttest: 0.8510690\tbest: 0.8510690 (105)\ttotal: 8m 39s\tremaining: 7m 41s\n",
            "106:\tlearn: 0.6265504\ttest: 0.8511375\tbest: 0.8510690 (105)\ttotal: 8m 44s\tremaining: 7m 36s\n",
            "107:\tlearn: 0.6260866\ttest: 0.8512680\tbest: 0.8510690 (105)\ttotal: 8m 49s\tremaining: 7m 31s\n",
            "108:\tlearn: 0.6225834\ttest: 0.8491463\tbest: 0.8491463 (108)\ttotal: 8m 54s\tremaining: 7m 26s\n",
            "109:\tlearn: 0.6193544\ttest: 0.8456352\tbest: 0.8456352 (109)\ttotal: 8m 59s\tremaining: 7m 21s\n",
            "110:\tlearn: 0.6183813\ttest: 0.8452170\tbest: 0.8452170 (110)\ttotal: 9m 4s\tremaining: 7m 16s\n",
            "111:\tlearn: 0.6177759\ttest: 0.8449377\tbest: 0.8449377 (111)\ttotal: 9m 9s\tremaining: 7m 12s\n",
            "112:\tlearn: 0.6169852\ttest: 0.8441963\tbest: 0.8441963 (112)\ttotal: 9m 14s\tremaining: 7m 7s\n",
            "113:\tlearn: 0.6153017\ttest: 0.8416616\tbest: 0.8416616 (113)\ttotal: 9m 19s\tremaining: 7m 2s\n",
            "114:\tlearn: 0.6147052\ttest: 0.8413332\tbest: 0.8413332 (114)\ttotal: 9m 24s\tremaining: 6m 57s\n",
            "115:\tlearn: 0.6143187\ttest: 0.8411364\tbest: 0.8411364 (115)\ttotal: 9m 29s\tremaining: 6m 52s\n",
            "116:\tlearn: 0.6132731\ttest: 0.8416026\tbest: 0.8411364 (115)\ttotal: 9m 34s\tremaining: 6m 47s\n",
            "117:\tlearn: 0.6117540\ttest: 0.8413544\tbest: 0.8411364 (115)\ttotal: 9m 39s\tremaining: 6m 42s\n",
            "118:\tlearn: 0.6113672\ttest: 0.8415416\tbest: 0.8411364 (115)\ttotal: 9m 44s\tremaining: 6m 38s\n",
            "119:\tlearn: 0.6109510\ttest: 0.8417340\tbest: 0.8411364 (115)\ttotal: 9m 49s\tremaining: 6m 33s\n",
            "120:\tlearn: 0.6102613\ttest: 0.8413933\tbest: 0.8411364 (115)\ttotal: 9m 54s\tremaining: 6m 28s\n",
            "121:\tlearn: 0.6096801\ttest: 0.8413089\tbest: 0.8411364 (115)\ttotal: 9m 59s\tremaining: 6m 23s\n",
            "122:\tlearn: 0.6066159\ttest: 0.8381121\tbest: 0.8381121 (122)\ttotal: 10m 4s\tremaining: 6m 18s\n",
            "123:\tlearn: 0.6054198\ttest: 0.8379521\tbest: 0.8379521 (123)\ttotal: 10m 9s\tremaining: 6m 13s\n",
            "124:\tlearn: 0.6040172\ttest: 0.8372608\tbest: 0.8372608 (124)\ttotal: 10m 14s\tremaining: 6m 8s\n",
            "125:\tlearn: 0.6031870\ttest: 0.8365414\tbest: 0.8365414 (125)\ttotal: 10m 19s\tremaining: 6m 3s\n",
            "126:\tlearn: 0.6029533\ttest: 0.8365599\tbest: 0.8365414 (125)\ttotal: 10m 24s\tremaining: 5m 59s\n",
            "127:\tlearn: 0.6004537\ttest: 0.8356839\tbest: 0.8356839 (127)\ttotal: 10m 29s\tremaining: 5m 54s\n",
            "128:\tlearn: 0.5996250\ttest: 0.8359524\tbest: 0.8356839 (127)\ttotal: 10m 34s\tremaining: 5m 49s\n",
            "129:\tlearn: 0.5974303\ttest: 0.8350604\tbest: 0.8350604 (129)\ttotal: 10m 39s\tremaining: 5m 44s\n",
            "130:\tlearn: 0.5968835\ttest: 0.8349226\tbest: 0.8349226 (130)\ttotal: 10m 44s\tremaining: 5m 39s\n",
            "131:\tlearn: 0.5963273\ttest: 0.8349584\tbest: 0.8349226 (130)\ttotal: 10m 49s\tremaining: 5m 34s\n",
            "132:\tlearn: 0.5916584\ttest: 0.8336220\tbest: 0.8336220 (132)\ttotal: 10m 54s\tremaining: 5m 29s\n",
            "133:\tlearn: 0.5911078\ttest: 0.8335335\tbest: 0.8335335 (133)\ttotal: 10m 59s\tremaining: 5m 24s\n",
            "134:\tlearn: 0.5903247\ttest: 0.8334899\tbest: 0.8334899 (134)\ttotal: 11m 4s\tremaining: 5m 19s\n",
            "135:\tlearn: 0.5895965\ttest: 0.8334271\tbest: 0.8334271 (135)\ttotal: 11m 9s\tremaining: 5m 14s\n",
            "136:\tlearn: 0.5887604\ttest: 0.8335689\tbest: 0.8334271 (135)\ttotal: 11m 14s\tremaining: 5m 10s\n",
            "137:\tlearn: 0.5869469\ttest: 0.8323043\tbest: 0.8323043 (137)\ttotal: 11m 19s\tremaining: 5m 5s\n",
            "138:\tlearn: 0.5862762\ttest: 0.8321240\tbest: 0.8321240 (138)\ttotal: 11m 24s\tremaining: 5m\n",
            "139:\tlearn: 0.5846883\ttest: 0.8318849\tbest: 0.8318849 (139)\ttotal: 11m 29s\tremaining: 4m 55s\n",
            "140:\tlearn: 0.5836858\ttest: 0.8313983\tbest: 0.8313983 (140)\ttotal: 11m 34s\tremaining: 4m 50s\n",
            "141:\tlearn: 0.5819319\ttest: 0.8308638\tbest: 0.8308638 (141)\ttotal: 11m 39s\tremaining: 4m 45s\n",
            "142:\tlearn: 0.5815692\ttest: 0.8306795\tbest: 0.8306795 (142)\ttotal: 11m 44s\tremaining: 4m 40s\n",
            "143:\tlearn: 0.5809705\ttest: 0.8303148\tbest: 0.8303148 (143)\ttotal: 11m 49s\tremaining: 4m 35s\n",
            "144:\tlearn: 0.5785759\ttest: 0.8290657\tbest: 0.8290657 (144)\ttotal: 11m 53s\tremaining: 4m 30s\n",
            "145:\tlearn: 0.5777300\ttest: 0.8294218\tbest: 0.8290657 (144)\ttotal: 11m 58s\tremaining: 4m 25s\n",
            "146:\tlearn: 0.5772234\ttest: 0.8296735\tbest: 0.8290657 (144)\ttotal: 12m 3s\tremaining: 4m 20s\n",
            "147:\tlearn: 0.5762003\ttest: 0.8288564\tbest: 0.8288564 (147)\ttotal: 12m 8s\tremaining: 4m 16s\n",
            "148:\tlearn: 0.5757390\ttest: 0.8288360\tbest: 0.8288360 (148)\ttotal: 12m 13s\tremaining: 4m 11s\n",
            "149:\tlearn: 0.5750135\ttest: 0.8288856\tbest: 0.8288360 (148)\ttotal: 12m 18s\tremaining: 4m 6s\n",
            "150:\tlearn: 0.5739599\ttest: 0.8290481\tbest: 0.8288360 (148)\ttotal: 12m 23s\tremaining: 4m 1s\n",
            "151:\tlearn: 0.5734148\ttest: 0.8291158\tbest: 0.8288360 (148)\ttotal: 12m 29s\tremaining: 3m 56s\n",
            "152:\tlearn: 0.5729475\ttest: 0.8292485\tbest: 0.8288360 (148)\ttotal: 12m 35s\tremaining: 3m 51s\n",
            "153:\tlearn: 0.5703629\ttest: 0.8295112\tbest: 0.8288360 (148)\ttotal: 12m 40s\tremaining: 3m 47s\n",
            "154:\tlearn: 0.5671196\ttest: 0.8274046\tbest: 0.8274046 (154)\ttotal: 12m 44s\tremaining: 3m 42s\n",
            "155:\tlearn: 0.5655437\ttest: 0.8253501\tbest: 0.8253501 (155)\ttotal: 12m 49s\tremaining: 3m 37s\n",
            "156:\tlearn: 0.5652893\ttest: 0.8255303\tbest: 0.8253501 (155)\ttotal: 12m 54s\tremaining: 3m 32s\n",
            "157:\tlearn: 0.5621444\ttest: 0.8234153\tbest: 0.8234153 (157)\ttotal: 12m 59s\tremaining: 3m 27s\n",
            "158:\tlearn: 0.5608360\ttest: 0.8236267\tbest: 0.8234153 (157)\ttotal: 13m 4s\tremaining: 3m 22s\n",
            "159:\tlearn: 0.5601897\ttest: 0.8233884\tbest: 0.8233884 (159)\ttotal: 13m 9s\tremaining: 3m 17s\n",
            "160:\tlearn: 0.5595809\ttest: 0.8229971\tbest: 0.8229971 (160)\ttotal: 13m 14s\tremaining: 3m 12s\n",
            "161:\tlearn: 0.5593010\ttest: 0.8228789\tbest: 0.8228789 (161)\ttotal: 13m 19s\tremaining: 3m 7s\n",
            "162:\tlearn: 0.5585734\ttest: 0.8225686\tbest: 0.8225686 (162)\ttotal: 13m 24s\tremaining: 3m 2s\n",
            "163:\tlearn: 0.5582664\ttest: 0.8227258\tbest: 0.8225686 (162)\ttotal: 13m 29s\tremaining: 2m 57s\n",
            "164:\tlearn: 0.5571321\ttest: 0.8229337\tbest: 0.8225686 (162)\ttotal: 13m 34s\tremaining: 2m 52s\n",
            "165:\tlearn: 0.5565085\ttest: 0.8225951\tbest: 0.8225686 (162)\ttotal: 13m 39s\tremaining: 2m 47s\n",
            "166:\tlearn: 0.5560637\ttest: 0.8226655\tbest: 0.8225686 (162)\ttotal: 13m 44s\tremaining: 2m 42s\n",
            "167:\tlearn: 0.5556958\ttest: 0.8227892\tbest: 0.8225686 (162)\ttotal: 13m 49s\tremaining: 2m 38s\n",
            "168:\tlearn: 0.5536563\ttest: 0.8217638\tbest: 0.8217638 (168)\ttotal: 13m 54s\tremaining: 2m 33s\n",
            "169:\tlearn: 0.5531232\ttest: 0.8215856\tbest: 0.8215856 (169)\ttotal: 13m 59s\tremaining: 2m 28s\n",
            "170:\tlearn: 0.5527225\ttest: 0.8216695\tbest: 0.8215856 (169)\ttotal: 14m 4s\tremaining: 2m 23s\n",
            "171:\tlearn: 0.5523997\ttest: 0.8216213\tbest: 0.8215856 (169)\ttotal: 14m 9s\tremaining: 2m 18s\n",
            "172:\tlearn: 0.5519473\ttest: 0.8212147\tbest: 0.8212147 (172)\ttotal: 14m 14s\tremaining: 2m 13s\n",
            "173:\tlearn: 0.5512823\ttest: 0.8210622\tbest: 0.8210622 (173)\ttotal: 14m 19s\tremaining: 2m 8s\n",
            "174:\tlearn: 0.5509165\ttest: 0.8211605\tbest: 0.8210622 (173)\ttotal: 14m 24s\tremaining: 2m 3s\n",
            "175:\tlearn: 0.5492304\ttest: 0.8202334\tbest: 0.8202334 (175)\ttotal: 14m 29s\tremaining: 1m 58s\n",
            "176:\tlearn: 0.5486492\ttest: 0.8201606\tbest: 0.8201606 (176)\ttotal: 14m 34s\tremaining: 1m 53s\n",
            "177:\tlearn: 0.5482730\ttest: 0.8200918\tbest: 0.8200918 (177)\ttotal: 14m 39s\tremaining: 1m 48s\n",
            "178:\tlearn: 0.5466216\ttest: 0.8185669\tbest: 0.8185669 (178)\ttotal: 14m 44s\tremaining: 1m 43s\n",
            "179:\tlearn: 0.5463263\ttest: 0.8185096\tbest: 0.8185096 (179)\ttotal: 14m 49s\tremaining: 1m 38s\n",
            "180:\tlearn: 0.5457013\ttest: 0.8184422\tbest: 0.8184422 (180)\ttotal: 14m 54s\tremaining: 1m 33s\n",
            "181:\tlearn: 0.5453012\ttest: 0.8187341\tbest: 0.8184422 (180)\ttotal: 14m 59s\tremaining: 1m 28s\n",
            "182:\tlearn: 0.5445589\ttest: 0.8186024\tbest: 0.8184422 (180)\ttotal: 15m 4s\tremaining: 1m 24s\n",
            "183:\tlearn: 0.5421889\ttest: 0.8169441\tbest: 0.8169441 (183)\ttotal: 15m 9s\tremaining: 1m 19s\n",
            "184:\tlearn: 0.5416674\ttest: 0.8169363\tbest: 0.8169363 (184)\ttotal: 15m 14s\tremaining: 1m 14s\n",
            "185:\tlearn: 0.5413340\ttest: 0.8169901\tbest: 0.8169363 (184)\ttotal: 15m 19s\tremaining: 1m 9s\n",
            "186:\tlearn: 0.5409010\ttest: 0.8171647\tbest: 0.8169363 (184)\ttotal: 15m 24s\tremaining: 1m 4s\n",
            "187:\tlearn: 0.5404890\ttest: 0.8171548\tbest: 0.8169363 (184)\ttotal: 15m 29s\tremaining: 59.3s\n",
            "188:\tlearn: 0.5402524\ttest: 0.8172500\tbest: 0.8169363 (184)\ttotal: 15m 34s\tremaining: 54.4s\n",
            "189:\tlearn: 0.5395076\ttest: 0.8172725\tbest: 0.8169363 (184)\ttotal: 15m 39s\tremaining: 49.4s\n",
            "190:\tlearn: 0.5388924\ttest: 0.8169052\tbest: 0.8169052 (190)\ttotal: 15m 44s\tremaining: 44.5s\n",
            "191:\tlearn: 0.5372861\ttest: 0.8169094\tbest: 0.8169052 (190)\ttotal: 15m 49s\tremaining: 39.6s\n",
            "192:\tlearn: 0.5365503\ttest: 0.8155827\tbest: 0.8155827 (192)\ttotal: 15m 54s\tremaining: 34.6s\n",
            "193:\tlearn: 0.5360668\ttest: 0.8156837\tbest: 0.8155827 (192)\ttotal: 15m 59s\tremaining: 29.7s\n",
            "194:\tlearn: 0.5358742\ttest: 0.8158901\tbest: 0.8155827 (192)\ttotal: 16m 4s\tremaining: 24.7s\n",
            "195:\tlearn: 0.5337995\ttest: 0.8144593\tbest: 0.8144593 (195)\ttotal: 16m 9s\tremaining: 19.8s\n",
            "196:\tlearn: 0.5333338\ttest: 0.8145505\tbest: 0.8144593 (195)\ttotal: 16m 14s\tremaining: 14.8s\n",
            "197:\tlearn: 0.5328314\ttest: 0.8145757\tbest: 0.8144593 (195)\ttotal: 16m 19s\tremaining: 9.89s\n",
            "198:\tlearn: 0.5324568\ttest: 0.8146878\tbest: 0.8144593 (195)\ttotal: 16m 24s\tremaining: 4.95s\n",
            "199:\tlearn: 0.5322092\ttest: 0.8147108\tbest: 0.8144593 (195)\ttotal: 16m 29s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.814459268\n",
            "bestIteration = 195\n",
            "\n",
            "Shrink model to first 196 iterations.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7764298093587522"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjmG392PSs_w",
        "colab_type": "text"
      },
      "source": [
        "# Saving the model\n",
        "I saved the best performing models using pickle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLeWrZ6iC6eS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save Model\n",
        "import pickle\n",
        "pickle.dump(rf, open('rf_model.sav', 'wb'))\n",
        "pickle.dump(catb, open('catb_model.sav', 'wb'))\n",
        "# To load model use: model = pickle.load(open(filename, 'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxLjDnoaT8MS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}